{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c149fe52",
   "metadata": {},
   "source": [
    "# Notebook Organizado\n",
    "Este notebook demonstra como configurar o ambiente e criar templates de prompt usando a biblioteca LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f596a",
   "metadata": {},
   "source": [
    "## Se√ß√£o 1: Configura√ß√£o do Ambiente\n",
    "Nesta se√ß√£o, s√£o carregadas as vari√°veis de ambiente e o modelo de linguagem √© inicializado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba2126",
   "metadata": {},
   "source": [
    "## Se√ß√£o 2: Exemplo de Uso do Template para Prompt\n",
    "Esta se√ß√£o mostra como criar um template de prompt din√¢mico para gerar uma piada sobre um tema espec√≠fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o modelo com o nome especificado nas vari√°veis de ambiente\n",
    "# Carregando vari√°veis de ambiente do arquivo .env\n",
    "# Exemplo 1: Setup do ambiente e bibliotecas\n",
    "# Importando bibliotecas necess√°rias\n",
    "#exemplo1.py\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando vari√°veis de ambiente do arquivo .env\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "OPENAI_MODEL_NAME = os.getenv('OPENAI_MODEL_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando o modelo com o nome especificado nas vari√°veis de ambiente\n",
    "llm = ChatOpenAI(temperature=0, model=OPENAI_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo 2A: Criando ChatPromptTemplate\n",
    "#### Exemplo 2A\n",
    "Esse exemplo mostra como criar um template de prompt para um modelo de linguagem, onde a vari√°vel topic √© inserida dinamicamente em uma template string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Por que os gatos s√£o p√©ssimos contadores de hist√≥rias?\\n\\nPorque eles sempre pulam para a parte \"miau\"! üò∏', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 16, 'total_tokens': 43, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-68216d14-2f65-4583-9681-f649c1f85047-0', usage_metadata={'input_tokens': 16, 'output_tokens': 27, 'total_tokens': 43, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo 2A: Criando um template de prompt para gerar piadas sobre o tema escolhido\n",
    "\n",
    "template = \"Conte uma piada engra√ßada sobre {topic}\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.invoke({\"topic\": \"Gatos\"})\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_multiple = '''Voc√™ √© um funcion√°rio de uma pizzaria vendendo diversos sabores de pizzas e refrigerantes, \n",
    "    sempre tirando d√∫vidas dos cliente e sendo extremamente educado,\n",
    "    seu nome √© {name}.\n",
    "    {menu}\n",
    "    Human: \"Ol√° boa noite, quais s√£o os sabores que temos dispon√≠vel para hoje?\"\n",
    "    Assistent: \n",
    "    '''   \n",
    "\n",
    "# Criando um cardapio e definindo o nome do nosso chatbot\n",
    "name = \"PizzaBot\"\n",
    "menu = '''\n",
    "### **Card√°pio de Pizzas**\n",
    "\n",
    "- **Pizza de Calabresa**\n",
    "  - Molho de tomate, fatias de calabresa, cebola, azeitonas e or√©gano.\n",
    "  \n",
    "- **Pizza de Frango com Catupiry**\n",
    "  - Molho de tomate, frango desfiado, requeij√£o Catupiry, milho e or√©gano.\n",
    "\n",
    "- **Pizza Portuguesa**\n",
    "  - Molho de tomate, presunto, ovo, cebola, azeitonas, ervilhas e or√©gano.\n",
    "---\n",
    "\n",
    "### **Refrigerantes**\n",
    "\n",
    "- **Coca-Cola**\n",
    "  - O refrigerante mais famoso do mundo, perfeito para acompanhar qualquer pizza.\n",
    "\n",
    "- **Guaran√° Antarctica**\n",
    "  - Refrigerante cl√°ssico de guaran√°, famoso no Brasil.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ol√°, boa noite! Temos dispon√≠veis hoje as seguintes op√ß√µes de pizzas:\\n\\n- **Pizza de Calabresa**: com molho de tomate, fatias de calabresa, cebola, azeitonas e or√©gano.\\n  \\n- **Pizza de Frango com Catupiry**: feita com molho de tomate, frango desfiado, requeij√£o Catupiry, milho e or√©gano.\\n\\n- **Pizza Portuguesa**: composta por molho de tomate, presunto, ovo, cebola, azeitonas, ervilhas e or√©gano.\\n\\nAl√©m disso, temos os refrigerantes:\\n\\n- **Coca-Cola**\\n- **Guaran√° Antarctica**\\n\\nSe precisar de mais informa√ß√µes ou ajuda para escolher, estou √† disposi√ß√£o!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 150, 'prompt_tokens': 233, 'total_tokens': 383, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-e26ca8c8-8127-44fa-a7f1-6b21e76b90e2-0', usage_metadata={'input_tokens': 233, 'output_tokens': 150, 'total_tokens': 383, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
    "prompt =prompt_multiple.invoke({\"name\":name, \"menu\":menu})\n",
    "result = llm.invoke(prompt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", '''Voc√™ √© um funcion√°rio de uma pizzaria vendendo diversos sabores de pizzas e refrigerantes, \n",
    "    sempre tirando d√∫vidas dos cliente e sendo extremamente educado,\n",
    "    seu nome √© {name}.\n",
    "    {menu}'''),\n",
    "    (\"human\", \"Ol√° boa noite, quais s√£o os sabores que temos dispon√≠vel para hoje?\"),\n",
    "])\n",
    "\n",
    "prompt_value = template.invoke({\"name\":name, \"menu\":menu})\n",
    "result = llm.invoke(prompt_value)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
